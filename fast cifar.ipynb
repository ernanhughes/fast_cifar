{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast CIFAR-10\n",
    "Copied from https://github.com/tysam-code/hlb-CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clears the interactive namespace, which means it removes all the variables, functions, imports, and other defined objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from functools import partial\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set global defaults (in this particular file) for convolutions\n",
    "default_conv_kwargs = {'kernel_size': 3, 'padding': 'same', 'bias': False}\n",
    "\n",
    "batchsize = 1024\n",
    "bias_scaler = 64\n",
    "# To replicate the ~95.79%-accuracy-in-110-seconds runs, you can change the base_depth from 64->128, train_epochs from 12.1->90, ['ema'] epochs 10->80, cutmix_size 3->10, and cutmix_epochs 6->80\n",
    "hyp = {\n",
    "    'opt': {\n",
    "        'bias_lr':        1.525 * bias_scaler/512, # TODO: Is there maybe a better way to express the bias and batchnorm scaling? :'))))\n",
    "        'non_bias_lr':    1.525 / 512,\n",
    "        'bias_decay':     6.687e-4 * batchsize/bias_scaler,\n",
    "        'non_bias_decay': 6.687e-4 * batchsize,\n",
    "        'scaling_factor': 1./9,\n",
    "        'percent_start': .23,\n",
    "        'loss_scale_scaler': 1./32, # * Regularizer inside the loss summing (range: ~1/512 - 16+). FP8 should help with this somewhat too, whenever it comes out. :)\n",
    "    },\n",
    "    'net': {\n",
    "        'whitening': {\n",
    "            'kernel_size': 2,\n",
    "            'num_examples': 50000,\n",
    "        },\n",
    "        'batch_norm_momentum': .4, # * Don't forget momentum is 1 - momentum here (due to a quirk in the original paper... >:( )\n",
    "        'cutmix_size': 3,\n",
    "        'cutmix_epochs': 6,\n",
    "        'pad_amount': 2,\n",
    "        'base_depth': 64 ## This should be a factor of 8 in some way to stay tensor core friendly\n",
    "    },\n",
    "    'misc': {\n",
    "        'ema': {\n",
    "            'epochs': 10, # Slight bug in that this counts only full epochs and then additionally runs the EMA for any fractional epochs at the end too\n",
    "            'decay_base': .95,\n",
    "            'decay_pow': 3.,\n",
    "            'every_n_steps': 5,\n",
    "        },\n",
    "        'train_epochs': 12.1,\n",
    "        'device': 'cuda',\n",
    "        'data_location': 'data.pt',\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "__spec__ = None\n",
    "if not os.path.exists(hyp[\"misc\"][\"data_location\"]):\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    cifar10 = CIFAR10(\"cifar10/\", download=True, train=True, transform=transform)\n",
    "    cifar10_eval = CIFAR10(\"cifar10/\", download=False, train=False, transform=transform)\n",
    "\n",
    "    # use the dataloader to get a single batch of all of the dataset items at once.\n",
    "    train_dataset_gpu_loader = DataLoader(\n",
    "        cifar10,\n",
    "        batch_size=len(cifar10),\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "    eval_dataset_gpu_loader = DataLoader(\n",
    "        cifar10_eval,\n",
    "        batch_size=len(cifar10_eval),\n",
    "        drop_last=True,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        persistent_workers=False,\n",
    "    )\n",
    "\n",
    "    train_dataset_gpu = {}\n",
    "    eval_dataset_gpu = {}\n",
    "\n",
    "    train_dataset_gpu[\"images\"], train_dataset_gpu[\"targets\"] = [\n",
    "        item.to(device=hyp[\"misc\"][\"device\"], non_blocking=True)\n",
    "        for item in next(iter(train_dataset_gpu_loader))\n",
    "    ]\n",
    "    eval_dataset_gpu[\"images\"], eval_dataset_gpu[\"targets\"] = [\n",
    "        item.to(device=hyp[\"misc\"][\"device\"], non_blocking=True)\n",
    "        for item in next(iter(eval_dataset_gpu_loader))\n",
    "    ]\n",
    "\n",
    "    # dynamically calculate the std and mean from the data. \n",
    "    # shortens the code and should help us adapt to new datasets\n",
    "    cifar10_std, cifar10_mean = torch.std_mean(\n",
    "        train_dataset_gpu[\"images\"], dim=(0, 2, 3)\n",
    "    )  \n",
    "\n",
    "    def batch_normalize_images(input_images, mean, std):\n",
    "        return (input_images - mean.view(1, -1, 1, 1)) / std.view(1, -1, 1, 1)\n",
    "\n",
    "    # preload with our mean and std\n",
    "    batch_normalize_images = partial(\n",
    "        batch_normalize_images, mean=cifar10_mean, std=cifar10_std\n",
    "    )\n",
    "\n",
    "    train_dataset_gpu[\"images\"] = batch_normalize_images(train_dataset_gpu[\"images\"])\n",
    "    eval_dataset_gpu[\"images\"] = batch_normalize_images(eval_dataset_gpu[\"images\"])\n",
    "\n",
    "    data = {\"train\": train_dataset_gpu, \"eval\": eval_dataset_gpu}\n",
    "\n",
    "    ## Convert dataset to FP16 now for the rest of the process....\n",
    "    data[\"train\"][\"images\"] = data[\"train\"][\"images\"].half().requires_grad_(False)\n",
    "    data[\"eval\"][\"images\"] = data[\"eval\"][\"images\"].half().requires_grad_(False)\n",
    "\n",
    "    # Convert this to one-hot to support the usage of cutmix \n",
    "    data[\"train\"][\"targets\"] = F.one_hot(data[\"train\"][\"targets\"]).half()\n",
    "    data[\"eval\"][\"targets\"] = F.one_hot(data[\"eval\"][\"targets\"]).half()\n",
    "\n",
    "    torch.save(data, hyp[\"misc\"][\"data_location\"])\n",
    "\n",
    "else:\n",
    "    ## This is effectively instantaneous, and takes us practically straight to where the dataloader-loaded dataset would be. :)\n",
    "    ## So as long as you run the above loading process once, and keep the file on the disc it's specified by default in the above\n",
    "    ## hyp dictionary, then we should be good. :)\n",
    "    data = torch.load(hyp[\"misc\"][\"data_location\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## As you'll note above and below, one difference is that we don't count loading the raw data to GPU since it's such a variable operation, and can sort of get in the way\n",
    "## of measuring other things. That said, measuring the preprocessing (outside of the padding) is still important to us.\n",
    "\n",
    "# Pad the GPU training dataset\n",
    "if hyp['net']['pad_amount'] > 0:\n",
    "    ## Uncomfortable shorthand, but basically we pad evenly on all _4_ sides with the pad_amount specified in the original dictionary\n",
    "    data['train']['images'] = F.pad(data['train']['images'], (hyp['net']['pad_amount'],)*4, 'reflect')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We might be able to fuse this weight and save some memory/runtime/etc, \n",
    "# since the fast version of the network might be able to do without somehow....\n",
    "class BatchNorm(nn.BatchNorm2d):\n",
    "    def __init__(self, num_features, eps=1e-12, momentum=hyp['net']['batch_norm_momentum'], weight=False, bias=True):\n",
    "        super().__init__(num_features, eps=eps, momentum=momentum)\n",
    "        self.weight.data.fill_(1.0)\n",
    "        self.bias.data.fill_(0.0)\n",
    "        self.weight.requires_grad = weight\n",
    "        self.bias.requires_grad = bias\n",
    "\n",
    "# Allows us to set default arguments for the whole convolution itself.\n",
    "# Having an outer class like this does add space and complexity but offers us\n",
    "# a ton of freedom when it comes to hacking in unique functionality for each layer type\n",
    "class Conv(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs = {**default_conv_kwargs, **kwargs}\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "class Linear(nn.Linear):\n",
    "    def __init__(self, *args, temperature=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.kwargs = kwargs\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.temperature is not None:\n",
    "            weight = self.weight * self.temperature\n",
    "        else:\n",
    "            weight = self.weight\n",
    "        return x @ weight.T\n",
    "\n",
    "# can hack any changes to each convolution group that you want directly in here\n",
    "class ConvGroup(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super().__init__()\n",
    "        self.channels_in  = channels_in\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv1 = Conv(channels_in,  channels_out)\n",
    "        self.conv2 = Conv(channels_out, channels_out)\n",
    "\n",
    "        self.norm1 = BatchNorm(channels_out)\n",
    "        self.norm2 = BatchNorm(channels_out)\n",
    "\n",
    "        self.activ = nn.GELU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.activ(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.activ(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class FastGlobalMaxPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Previously was chained torch.max calls.\n",
    "        # requires less time than AdaptiveMax2dPooling -- about ~.3s for the entire run, in fact (which is pretty significant! :O :D :O :O <3 <3 <3 <3)\n",
    "        return torch.amax(x, dim=(2,3)) # Global maximum pooling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches(x, patch_shape=(3, 3), dtype=torch.float32):\n",
    "    # This uses the unfold operation (https://pytorch.org/docs/stable/generated/torch.nn.functional.unfold.html?highlight=unfold#torch.nn.functional.unfold)\n",
    "    # to extract a _view_ (i.e., there's no data copied here) of blocks in the input tensor. We have to do it twice -- once horizontally, once vertically. Then\n",
    "    # from that, we get our kernel_size*kernel_size patches to later calculate the statistics for the whitening tensor on :D\n",
    "    c, (h, w) = x.shape[1], patch_shape\n",
    "    return x.unfold(2,h,1).unfold(3,w,1).transpose(1,3).reshape(-1,c,h,w).to(dtype) # TODO: Annotate?\n",
    "\n",
    "def get_whitening_parameters(patches):\n",
    "    # As a high-level summary, we're basically finding the high-dimensional oval that best fits the data here.\n",
    "    # We can then later use this information to map the input information to a nicely distributed sphere, where also\n",
    "    # the most significant features of the inputs each have their own axis. This significantly cleans things up for the\n",
    "    # rest of the neural network and speeds up training.\n",
    "    n,c,h,w = patches.shape\n",
    "    est_covariance = torch.cov(patches.view(n, c*h*w).t())\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(est_covariance, UPLO='U') # this is the same as saying we want our eigenvectors, with the specification that the matrix be an upper triangular matrix (instead of a lower-triangular matrix)\n",
    "    return eigenvalues.flip(0).view(-1, 1, 1, 1), eigenvectors.t().reshape(c*h*w,c,h,w).flip(0)\n",
    "\n",
    "# Run this over the training set to calculate the patch statistics, then set the initial convolution as a non-learnable 'whitening' layer\n",
    "def init_whitening_conv(layer, train_set=None, num_examples=None, previous_block_data=None, pad_amount=None, freeze=True, whiten_splits=None):\n",
    "    if train_set is not None and previous_block_data is None:\n",
    "        if pad_amount > 0:\n",
    "            previous_block_data = train_set[:num_examples,:,pad_amount:-pad_amount,pad_amount:-pad_amount] # if it's none, we're at the beginning of our network.\n",
    "        else:\n",
    "            previous_block_data = train_set[:num_examples,:,:,:]\n",
    "\n",
    "    # chunking code to save memory for smaller-memory-size (generally consumer) GPUs\n",
    "    if whiten_splits is None:\n",
    "         previous_block_data_split = [previous_block_data] # If we're whitening in one go, then put it in a list for simplicity to reuse the logic below\n",
    "    else:\n",
    "         previous_block_data_split = previous_block_data.split(whiten_splits, dim=0) # Otherwise, we split this into different chunks to keep things manageable\n",
    "\n",
    "    eigenvalue_list, eigenvector_list = [], []\n",
    "    for data_split in previous_block_data_split:\n",
    "        eigenvalues, eigenvectors = get_whitening_parameters(get_patches(data_split, patch_shape=layer.weight.data.shape[2:]))\n",
    "        eigenvalue_list.append(eigenvalues)\n",
    "        eigenvector_list.append(eigenvectors)\n",
    "\n",
    "    eigenvalues  = torch.stack(eigenvalue_list,  dim=0).mean(0)\n",
    "    eigenvectors = torch.stack(eigenvector_list, dim=0).mean(0)\n",
    "    # i believe the eigenvalues and eigenvectors come out in float32 for this because we implicitly cast it to float32 in the patches function (for numerical stability)\n",
    "    set_whitening_conv(layer, eigenvalues.to(dtype=layer.weight.dtype), eigenvectors.to(dtype=layer.weight.dtype), freeze=freeze)\n",
    "    data = layer(previous_block_data.to(dtype=layer.weight.dtype))\n",
    "    return data\n",
    "\n",
    "def set_whitening_conv(conv_layer, eigenvalues, eigenvectors, eps=1e-2, freeze=True):\n",
    "    shape = conv_layer.weight.data.shape\n",
    "    eigenvectors_sliced = (eigenvectors/torch.sqrt(eigenvalues+eps))[-shape[0]:, :, :, :] # set the first n filters of the weight data to the top n significant (sorted by importance) filters from the eigenvectors\n",
    "    conv_layer.weight.data = torch.cat((eigenvectors_sliced, -eigenvectors_sliced), dim=0)\n",
    "    ## We don't want to train this, since this is implicitly whitening over the whole dataset\n",
    "    ## For more info, see David Page's original blogposts (link in the README.md as of this commit.)\n",
    "    if freeze: \n",
    "        conv_layer.weight.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = 2. ## You can play with this on your own if you want, for the first beta I wanted to keep things simple (for now) and leave it out of the hyperparams dict\n",
    "depths = {\n",
    "    'init':   round(scaler**-1*hyp['net']['base_depth']), # 32  w/ scaler at base value\n",
    "    'block1': round(scaler** 0*hyp['net']['base_depth']), # 64  w/ scaler at base value\n",
    "    'block2': round(scaler** 2*hyp['net']['base_depth']), # 256 w/ scaler at base value\n",
    "    'block3': round(scaler** 3*hyp['net']['base_depth']), # 512 w/ scaler at base value\n",
    "    'num_classes': 10\n",
    "}\n",
    "\n",
    "class SpeedyConvNet(nn.Module):\n",
    "    def __init__(self, network_dict):\n",
    "        super().__init__()\n",
    "        self.net_dict = network_dict # flexible, defined in the make_net function\n",
    "\n",
    "    # This allows you to customize/change the execution order of the network as needed.\n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            x = torch.cat((x, torch.flip(x, (-1,))))\n",
    "        x = self.net_dict['initial_block']['whiten'](x)\n",
    "        x = self.net_dict['initial_block']['activation'](x)\n",
    "        x = self.net_dict['conv_group_1'](x)\n",
    "        x = self.net_dict['conv_group_2'](x)\n",
    "        x = self.net_dict['conv_group_3'](x)\n",
    "        x = self.net_dict['pooling'](x)\n",
    "        x = self.net_dict['linear'](x)\n",
    "        if not self.training:\n",
    "            # Average the predictions from the lr-flipped inputs during eval\n",
    "            orig, flipped = x.split(x.shape[0]//2, dim=0)\n",
    "            x = .5 * orig + .5 * flipped\n",
    "        return x\n",
    "\n",
    "def make_net():\n",
    "    # TODO: A way to make this cleaner??\n",
    "    # Note, you have to specify any arguments overlapping with defaults (i.e. everything but in/out depths) as kwargs so that they are properly overridden (TODO cleanup somehow?)\n",
    "    whiten_conv_depth = 3*hyp['net']['whitening']['kernel_size']**2\n",
    "    network_dict = nn.ModuleDict({\n",
    "        'initial_block': nn.ModuleDict({\n",
    "            'whiten': Conv(3, whiten_conv_depth, kernel_size=hyp['net']['whitening']['kernel_size'], padding=0),\n",
    "            'activation': nn.GELU(),\n",
    "        }),\n",
    "        'conv_group_1': ConvGroup(2*whiten_conv_depth, depths['block1']),\n",
    "        'conv_group_2': ConvGroup(depths['block1'],    depths['block2']),\n",
    "        'conv_group_3': ConvGroup(depths['block2'],    depths['block3']),\n",
    "        'pooling': FastGlobalMaxPooling(),\n",
    "        'linear': Linear(depths['block3'], depths['num_classes'], bias=False, temperature=hyp['opt']['scaling_factor']),\n",
    "    })\n",
    "\n",
    "    net = SpeedyConvNet(network_dict)\n",
    "    net = net.to(hyp['misc']['device'])\n",
    "    net = net.to(memory_format=torch.channels_last) # to appropriately use tensor cores/avoid thrash while training\n",
    "    net.train()\n",
    "    net.half() # Convert network to half before initializing the initial whitening layer.\n",
    "\n",
    "\n",
    "    ## Initialize the whitening convolution\n",
    "    with torch.no_grad():\n",
    "        # Initialize the first layer to be fixed weights that whiten the expected input values of the network be on the unit hypersphere. (i.e. their...average vector length is 1.?, IIRC)\n",
    "        init_whitening_conv(net.net_dict['initial_block']['whiten'],\n",
    "                            data['train']['images'].index_select(0, torch.randperm(data['train']['images'].shape[0], device=data['train']['images'].device)),\n",
    "                            num_examples=hyp['net']['whitening']['num_examples'],\n",
    "                            pad_amount=hyp['net']['pad_amount'],\n",
    "                            whiten_splits=5000) ## Hardcoded for now while we figure out the optimal whitening number\n",
    "                                                ## If you're running out of memory (OOM) feel free to decrease this, but\n",
    "                                                ## the index lookup in the dataloader may give you some trouble depending\n",
    "                                                ## upon exactly how memory-limited you are\n",
    "\n",
    "\n",
    "        for layer_name in net.net_dict.keys():\n",
    "            if 'conv_group' in layer_name:\n",
    "                # Create an implicit residual via a dirac-initialized tensor\n",
    "                dirac_weights_in = torch.nn.init.dirac_(torch.empty_like(net.net_dict[layer_name].conv1.weight))\n",
    "\n",
    "                # Add the implicit residual to the already-initialized convolutional transition layer.\n",
    "                # One can use more sophisticated initializations, but this one appeared worked best in testing.\n",
    "                # What this does is brings up the features from the previous residual block virtually, so not only \n",
    "                # do we have residual information flow within each block, we have a nearly direct connection from\n",
    "                # the early layers of the network to the loss function.\n",
    "                std_pre, mean_pre = torch.std_mean(net.net_dict[layer_name].conv1.weight.data)\n",
    "                net.net_dict[layer_name].conv1.weight.data = net.net_dict[layer_name].conv1.weight.data + dirac_weights_in \n",
    "                std_post, mean_post = torch.std_mean(net.net_dict[layer_name].conv1.weight.data)\n",
    "\n",
    "                # Renormalize the weights to match the original initialization statistics\n",
    "                net.net_dict[layer_name].conv1.weight.data.sub_(mean_post).div_(std_post).mul_(std_pre).add_(mean_pre)\n",
    "\n",
    "                ## We do the same for the second layer in each convolution group block, since this only\n",
    "                ## adds a simple multiplier to the inputs instead of the noise of a randomly-initialized\n",
    "                ## convolution. This can be easily scaled down by the network, and the weights can more easily\n",
    "                ## pivot in whichever direction they need to go now.\n",
    "                ## The reason that I believe that this works so well is because a combination of MaxPool2d\n",
    "                ## and the nn.GeLU function's positive bias encouraging values towards the nearly-linear\n",
    "                ## region of the GeLU activation function at network initialization. I am not currently\n",
    "                ## sure about this, however, it will require some more investigation. For now -- it works! D:\n",
    "                torch.nn.init.dirac_(net.net_dict[layer_name].conv2.weight)\n",
    "\n",
    "\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is actually (I believe) a pretty clean implementation of how to do something like this, since shifted-square masks unique to each depth-channel can actually be rather\n",
    "## tricky in practice. That said, if there's a better way, please do feel free to submit it! This can be one of the harder parts of the code to understand (though I personally get\n",
    "## stuck on the fold/unfold process for the lower-level convolution calculations.\n",
    "def make_random_square_masks(inputs, mask_size):\n",
    "    ##### TODO: Double check that this properly covers the whole range of values. :'( :')\n",
    "    if mask_size == 0:\n",
    "        return None # no need to cutout or do anything like that since the patch_size is set to 0\n",
    "    is_even = int(mask_size % 2 == 0)\n",
    "    in_shape = inputs.shape\n",
    "\n",
    "    # seed centers of squares to cutout boxes from, in one dimension each\n",
    "    mask_center_y = torch.empty(in_shape[0], dtype=torch.long, device=inputs.device).random_(mask_size//2-is_even, in_shape[-2]-mask_size//2-is_even)\n",
    "    mask_center_x = torch.empty(in_shape[0], dtype=torch.long, device=inputs.device).random_(mask_size//2-is_even, in_shape[-1]-mask_size//2-is_even)\n",
    "\n",
    "    # measure distance, using the center as a reference point\n",
    "    to_mask_y_dists = torch.arange(in_shape[-2], device=inputs.device).view(1, 1, in_shape[-2], 1) - mask_center_y.view(-1, 1, 1, 1)\n",
    "    to_mask_x_dists = torch.arange(in_shape[-1], device=inputs.device).view(1, 1, 1, in_shape[-1]) - mask_center_x.view(-1, 1, 1, 1)\n",
    "\n",
    "    to_mask_y = (to_mask_y_dists >= (-(mask_size // 2) + is_even)) * (to_mask_y_dists <= mask_size // 2)\n",
    "    to_mask_x = (to_mask_x_dists >= (-(mask_size // 2) + is_even)) * (to_mask_x_dists <= mask_size // 2)\n",
    "\n",
    "    final_mask = to_mask_y * to_mask_x ## Turn (y by 1) and (x by 1) boolean masks into (y by x) masks through multiplication. Their intersection is square, hurray! :D\n",
    "\n",
    "    return final_mask\n",
    "\n",
    "\n",
    "def batch_cutmix(inputs, targets, patch_size):\n",
    "    with torch.no_grad():\n",
    "        batch_permuted = torch.randperm(inputs.shape[0], device='cuda')\n",
    "        cutmix_batch_mask = make_random_square_masks(inputs, patch_size)\n",
    "        if cutmix_batch_mask is None:\n",
    "            return inputs, targets # if the mask is None, then that's because the patch size was set to 0 and we will not be using cutmix today.\n",
    "        # We draw other samples from inside of the same batch\n",
    "        cutmix_batch = torch.where(cutmix_batch_mask, torch.index_select(inputs, 0, batch_permuted), inputs)\n",
    "        cutmix_targets = torch.index_select(targets, 0, batch_permuted)\n",
    "        # Get the percentage of each target to mix for the labels by the % proportion of pixels in the mix\n",
    "        portion_mixed = float(patch_size**2)/(inputs.shape[-2]*inputs.shape[-1])\n",
    "        cutmix_labels = portion_mixed * cutmix_targets + (1. - portion_mixed) * targets\n",
    "        return cutmix_batch, cutmix_labels\n",
    "\n",
    "def batch_crop(inputs, crop_size):\n",
    "    with torch.no_grad():\n",
    "        crop_mask_batch = make_random_square_masks(inputs, crop_size)\n",
    "        cropped_batch = torch.masked_select(inputs, crop_mask_batch).view(inputs.shape[0], inputs.shape[1], crop_size, crop_size)\n",
    "        return cropped_batch\n",
    "\n",
    "def batch_flip_lr(batch_images, flip_chance=.5):\n",
    "    with torch.no_grad():\n",
    "        # TODO: Is there a more elegant way to do this? :') :'((((\n",
    "        return torch.where(torch.rand_like(batch_images[:, 0, 0, 0].view(-1, 1, 1, 1)) < flip_chance, torch.flip(batch_images, (-1,)), batch_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkEMA(nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super().__init__() # init the parent module so this module is registered properly\n",
    "        self.net_ema = copy.deepcopy(net).eval().requires_grad_(False) # copy the model\n",
    "\n",
    "    def update(self, current_net, decay):\n",
    "        with torch.no_grad():\n",
    "            for ema_net_parameter, (parameter_name, incoming_net_parameter) in zip(self.net_ema.state_dict().values(), current_net.state_dict().items()): # potential bug: assumes that the network architectures don't change during training (!!!!)\n",
    "                if incoming_net_parameter.dtype in (torch.half, torch.float):\n",
    "                    ema_net_parameter.mul_(decay).add_(incoming_net_parameter.detach().mul(1. - decay)) # update the ema values in place, similar to how optimizer momentum is coded\n",
    "                    # And then we also copy the parameters back to the network, similarly to the Lookahead optimizer (but with a much more aggressive-at-the-end schedule)\n",
    "                    if not ('norm' in parameter_name and 'weight' in parameter_name) and not 'whiten' in parameter_name:\n",
    "                        incoming_net_parameter.copy_(ema_net_parameter.detach())\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        with torch.no_grad():\n",
    "            return self.net_ema(inputs)\n",
    "\n",
    "# TODO: Could we jit this in the (more distant) future? :)\n",
    "@torch.no_grad()\n",
    "def get_batches(data_dict, key, batchsize, epoch_fraction=1., cutmix_size=None):\n",
    "    num_epoch_examples = len(data_dict[key]['images'])\n",
    "    shuffled = torch.randperm(num_epoch_examples, device='cuda')\n",
    "    if epoch_fraction < 1:\n",
    "        shuffled = shuffled[:batchsize * round(epoch_fraction * shuffled.shape[0]/batchsize)] # TODO: Might be slightly inaccurate, let's fix this later... :) :D :confetti: :fireworks:\n",
    "        num_epoch_examples = shuffled.shape[0]\n",
    "    crop_size = 32\n",
    "    ## Here, we prep the dataset by applying all data augmentations in batches ahead of time before each epoch, then we return an iterator below\n",
    "    ## that iterates in chunks over with a random derangement (i.e. shuffled indices) of the individual examples. So we get perfectly-shuffled\n",
    "    ## batches (which skip the last batch if it's not a full batch), but everything seems to be (and hopefully is! :D) properly shuffled. :)\n",
    "    if key == 'train':\n",
    "        images = batch_crop(data_dict[key]['images'], crop_size) # TODO: hardcoded image size for now?\n",
    "        images = batch_flip_lr(images)\n",
    "        images, targets = batch_cutmix(images, data_dict[key]['targets'], patch_size=cutmix_size)\n",
    "    else:\n",
    "        images = data_dict[key]['images']\n",
    "        targets = data_dict[key]['targets']\n",
    "\n",
    "    # Send the images to an (in beta) channels_last to help improve tensor core occupancy (and reduce NCHW <-> NHWC thrash) during training\n",
    "    images = images.to(memory_format=torch.channels_last)\n",
    "    for idx in range(num_epoch_examples // batchsize):\n",
    "        if not (idx+1)*batchsize > num_epoch_examples: ## Use the shuffled randperm to assemble individual items into a minibatch\n",
    "            yield images.index_select(0, shuffled[idx*batchsize:(idx+1)*batchsize]), \\\n",
    "                  targets.index_select(0, shuffled[idx*batchsize:(idx+1)*batchsize]) ## Each item is only used/accessed by the network once per epoch. :D\n",
    "\n",
    "\n",
    "def init_split_parameter_dictionaries(network):\n",
    "    params_non_bias = {'params': [], 'lr': hyp['opt']['non_bias_lr'], 'momentum': .85, 'nesterov': True, 'weight_decay': hyp['opt']['non_bias_decay'], 'foreach': True}\n",
    "    params_bias     = {'params': [], 'lr': hyp['opt']['bias_lr'],     'momentum': .85, 'nesterov': True, 'weight_decay': hyp['opt']['bias_decay'], 'foreach': True}\n",
    "\n",
    "    for name, p in network.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            if 'bias' in name:\n",
    "                params_bias['params'].append(p)\n",
    "            else:\n",
    "                params_non_bias['params'].append(p)\n",
    "    return params_non_bias, params_bias\n",
    "\n",
    "\n",
    "## Hey look, it's the soft-targets/label-smoothed loss! Native to PyTorch. Now, _that_ is pretty cool, and simplifies things a lot, to boot! :D :)\n",
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.2, reduction='none')\n",
    "\n",
    "logging_columns_list = ['epoch', 'train_loss', 'val_loss', 'train_acc', 'val_acc', 'ema_val_acc', 'total_time_seconds']\n",
    "# define the printing function and print the column heads\n",
    "def print_training_details(columns_list, separator_left='|  ', separator_right='  ', final=\"|\", column_heads_only=False, is_final_entry=False):\n",
    "    print_string = \"\"\n",
    "    if column_heads_only:\n",
    "        for column_head_name in columns_list:\n",
    "            print_string += separator_left + column_head_name + separator_right\n",
    "        print_string += final\n",
    "        print('-'*(len(print_string))) # print the top bar\n",
    "        print(print_string)\n",
    "        print('-'*(len(print_string))) # print the bottom bar\n",
    "    else:\n",
    "        for column_value in columns_list:\n",
    "            print_string += separator_left + column_value + separator_right\n",
    "        print_string += final\n",
    "        print(print_string)\n",
    "    if is_final_entry:\n",
    "        print('-'*(len(print_string))) # print the final output bar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------\n",
      "|  epoch  |  train_loss  |  val_loss  |  train_acc  |  val_acc  |  ema_val_acc  |  total_time_seconds  |\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "|      0  |      2.3105  |    1.5137  |     0.0947  |   0.6512  |               |              3.0911  |\n",
      "|      1  |      1.4609  |    1.2773  |     0.6934  |   0.8032  |               |              6.0199  |\n",
      "|      2  |      1.2402  |    1.2876  |     0.8232  |   0.7873  |       0.7966  |              8.9684  |\n",
      "|      3  |      1.2344  |    1.2325  |     0.8174  |   0.8206  |       0.8538  |             11.9115  |\n",
      "|      4  |      1.1572  |    1.1830  |     0.8672  |   0.8477  |       0.8477  |             14.8535  |\n",
      "|      5  |      1.1123  |    1.1656  |     0.8965  |   0.8573  |       0.8799  |             17.8024  |\n",
      "|      6  |      1.0908  |    1.1477  |     0.9062  |   0.8638  |       0.8860  |             20.7453  |\n",
      "|      7  |      1.0928  |    1.0875  |     0.9023  |   0.8961  |       0.9082  |             23.7069  |\n",
      "|      8  |      1.0625  |    1.0809  |     0.9277  |   0.9030  |       0.9192  |             26.6752  |\n",
      "|      9  |      1.0381  |    1.0255  |     0.9385  |   0.9288  |       0.9288  |             29.6324  |\n",
      "|     10  |      0.9751  |    1.0235  |     0.9795  |   0.9284  |       0.9369  |             32.5899  |\n",
      "|     11  |      0.9644  |    1.0010  |     0.9863  |   0.9396  |       0.9402  |             35.5600  |\n",
      "|     12  |      0.9502  |    0.9998  |     0.9883  |   0.9394  |       0.9401  |             35.9385  |\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "Final ema accuracy achieved: 0.9400999546051025\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initializing constants for the whole run.\n",
    "net_ema = None ## Reset any existing network emas, we want to have _something_ to check for existence so we can initialize the EMA right from where the network is during training\n",
    "                ## (as opposed to initializing the network_ema from the randomly-initialized starter network, then forcing it to play catch-up all of a sudden in the last several epochs)\n",
    "\n",
    "total_time_seconds = 0.\n",
    "current_steps = 0.\n",
    "\n",
    "# TODO: Doesn't currently account for partial epochs really (since we're not doing \"real\" epochs across the whole batchsize)....\n",
    "num_steps_per_epoch      = len(data['train']['images']) // batchsize\n",
    "total_train_steps        = math.ceil(num_steps_per_epoch * hyp['misc']['train_epochs'])\n",
    "ema_epoch_start          = math.floor(hyp['misc']['train_epochs']) - hyp['misc']['ema']['epochs']\n",
    "\n",
    "## I believe this wasn't logged, but the EMA update power is adjusted by being raised to the power of the number of \"every n\" steps\n",
    "## to somewhat accomodate for whatever the expected information intake rate is. The tradeoff I believe, though, is that this is to some degree noisier as we\n",
    "## are intaking fewer samples of our distribution-over-time, with a higher individual weight each. This can be good or bad depending upon what we want.\n",
    "projected_ema_decay_val  = hyp['misc']['ema']['decay_base'] ** hyp['misc']['ema']['every_n_steps']\n",
    "\n",
    "# Adjust pct_start based upon how many epochs we need to finetune the ema at a low lr for\n",
    "pct_start = hyp['opt']['percent_start'] #* (total_train_steps/(total_train_steps - num_low_lr_steps_for_ema))\n",
    "\n",
    "# Get network\n",
    "net = make_net()\n",
    "\n",
    "## Stowing the creation of these into a helper function to make things a bit more readable....\n",
    "non_bias_params, bias_params = init_split_parameter_dictionaries(net)\n",
    "\n",
    "# One optimizer for the regular network, and one for the biases. This allows us to use the superconvergence onecycle training policy for our networks....\n",
    "opt = torch.optim.SGD(**non_bias_params)\n",
    "opt_bias = torch.optim.SGD(**bias_params)\n",
    "\n",
    "## Not the most intuitive, but this basically takes us from ~0 to max_lr at the point pct_start, then down to .1 * max_lr at the end (since 1e16 * 1e-15 = .1 --\n",
    "##   This quirk is because the final lr value is calculated from the starting lr value and not from the maximum lr value set during training)\n",
    "initial_div_factor = 1e16 # basically to make the initial lr ~0 or so :D\n",
    "final_lr_ratio = .07 # Actually pretty important, apparently!\n",
    "lr_sched      = torch.optim.lr_scheduler.OneCycleLR(opt,  max_lr=non_bias_params['lr'], pct_start=pct_start, div_factor=initial_div_factor, final_div_factor=1./(initial_div_factor*final_lr_ratio), total_steps=total_train_steps, anneal_strategy='linear', cycle_momentum=False)\n",
    "lr_sched_bias = torch.optim.lr_scheduler.OneCycleLR(opt_bias, max_lr=bias_params['lr'], pct_start=pct_start, div_factor=initial_div_factor, final_div_factor=1./(initial_div_factor*final_lr_ratio), total_steps=total_train_steps, anneal_strategy='linear', cycle_momentum=False)\n",
    "\n",
    "## For accurately timing GPU code\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "torch.cuda.synchronize() ## clean up any pre-net setup operations\n",
    "\n",
    "print_training_details(logging_columns_list, column_heads_only=True) ## print out the training column heads before we print the actual content for each run.\n",
    "\n",
    "if True: ## Sometimes we need a conditional/for loop here, this is placed to save the trouble of needing to indent\n",
    "    for epoch in range(math.ceil(hyp['misc']['train_epochs'])):\n",
    "        #################\n",
    "        # Training Mode #\n",
    "        #################\n",
    "        torch.cuda.synchronize()\n",
    "        starter.record()\n",
    "        net.train()\n",
    "\n",
    "        loss_train = None\n",
    "        accuracy_train = None\n",
    "\n",
    "        cutmix_size = hyp['net']['cutmix_size'] if epoch >= hyp['misc']['train_epochs'] - hyp['net']['cutmix_epochs'] else 0\n",
    "        epoch_fraction = 1 if epoch + 1 < hyp['misc']['train_epochs'] else hyp['misc']['train_epochs'] % 1 # We need to know if we're running a partial epoch or not.\n",
    "\n",
    "        for epoch_step, (inputs, targets) in enumerate(get_batches(data, key='train', batchsize=batchsize, epoch_fraction=epoch_fraction, cutmix_size=cutmix_size)):\n",
    "            ## Run everything through the network\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss_batchsize_scaler = 512/batchsize # to scale to keep things at a relatively similar amount of regularization when we change our batchsize since we're summing over the whole batch\n",
    "            ## If you want to add other losses or hack around with the loss, you can do that here.\n",
    "            loss = loss_fn(outputs, targets).mul(hyp['opt']['loss_scale_scaler']*loss_batchsize_scaler).sum().div(hyp['opt']['loss_scale_scaler']) ## Note, as noted in the original blog posts, the summing here does a kind of loss scaling\n",
    "                                                    ## (and is thus batchsize dependent as a result). This can be somewhat good or bad, depending...\n",
    "\n",
    "            # we only take the last-saved accs and losses from train\n",
    "            if epoch_step % 50 == 0:\n",
    "                train_acc = (outputs.detach().argmax(-1) == targets.argmax(-1)).float().mean().item()\n",
    "                train_loss = loss.detach().cpu().item()/(batchsize*loss_batchsize_scaler)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            ## Step for each optimizer, in turn.\n",
    "            opt.step()\n",
    "            opt_bias.step()\n",
    "\n",
    "            # We only want to step the lr_schedulers while we have training steps to consume. Otherwise we get a not-so-friendly error from PyTorch\n",
    "            lr_sched.step()\n",
    "            lr_sched_bias.step()\n",
    "\n",
    "            ## Using 'set_to_none' I believe is slightly faster (albeit riskier w/ funky gradient update workflows) than under the default 'set to zero' method\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            opt_bias.zero_grad(set_to_none=True)\n",
    "            current_steps += 1\n",
    "\n",
    "            if epoch >= ema_epoch_start and current_steps % hyp['misc']['ema']['every_n_steps'] == 0:          \n",
    "                ## Initialize the ema from the network at this point in time if it does not already exist.... :D\n",
    "                if net_ema is None: # don't snapshot the network yet if so!\n",
    "                    net_ema = NetworkEMA(net)\n",
    "                    continue\n",
    "                # We warm up our ema's decay/momentum value over training exponentially according to the hyp config dictionary (this lets us move fast, then average strongly at the end).\n",
    "                net_ema.update(net, decay=projected_ema_decay_val*(current_steps/total_train_steps)**hyp['misc']['ema']['decay_pow'])\n",
    "\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        total_time_seconds += 1e-3 * starter.elapsed_time(ender)\n",
    "\n",
    "        ####################\n",
    "        # Evaluation  Mode #\n",
    "        ####################\n",
    "        net.eval()\n",
    "\n",
    "        eval_batchsize = 2500\n",
    "        assert data['eval']['images'].shape[0] % eval_batchsize == 0, \"Error: The eval batchsize must evenly divide the eval dataset (for now, we don't have drop_remainder implemented yet).\"\n",
    "        loss_list_val, acc_list, acc_list_ema = [], [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in get_batches(data, key='eval', batchsize=eval_batchsize):\n",
    "                if epoch >= ema_epoch_start:\n",
    "                    outputs = net_ema(inputs)\n",
    "                    acc_list_ema.append((outputs.argmax(-1) == targets.argmax(-1)).float().mean())\n",
    "                outputs = net(inputs)\n",
    "                loss_list_val.append(loss_fn(outputs, targets).float().mean())\n",
    "                acc_list.append((outputs.argmax(-1) == targets.argmax(-1)).float().mean())\n",
    "                \n",
    "            val_acc = torch.stack(acc_list).mean().item()\n",
    "            ema_val_acc = None\n",
    "            # TODO: We can fuse these two operations (just above and below) all-together like :D :))))\n",
    "            if epoch >= ema_epoch_start:\n",
    "                ema_val_acc = torch.stack(acc_list_ema).mean().item()\n",
    "\n",
    "            val_loss = torch.stack(loss_list_val).mean().item()\n",
    "        # We basically need to look up local variables by name so we can have the names, so we can pad to the proper column width.\n",
    "        ## Printing stuff in the terminal can get tricky and this used to use an outside library, but some of the required stuff seemed even\n",
    "        ## more heinous than this, unfortunately. So we switched to the \"more simple\" version of this!\n",
    "        format_for_table = lambda x, locals: (f\"{locals[x]}\".rjust(len(x))) \\\n",
    "                                                if type(locals[x]) == int else \"{:0.4f}\".format(locals[x]).rjust(len(x)) \\\n",
    "                                            if locals[x] is not None \\\n",
    "                                            else \" \"*len(x)\n",
    "\n",
    "        # Print out our training details (sorry for the complexity, the whole logging business here is a bit of a hot mess once the columns need to be aligned and such....)\n",
    "        ## We also check to see if we're in our final epoch so we can print the 'bottom' of the table for each round.\n",
    "        print_training_details(list(map(partial(format_for_table, locals=locals()), logging_columns_list)), is_final_entry=(epoch >= math.ceil(hyp['misc']['train_epochs'] - 1)))\n",
    "ema_val_acc # Return the final ema accuracy achieved (not using the 'best accuracy' selection strategy, which I think is okay here....)\n",
    "\n",
    "print(f'Final ema accuracy achieved: {ema_val_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
